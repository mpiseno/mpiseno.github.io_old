---
layout: about
title: about
permalink: /
description: <a href="/assets/pdf/michael_piseno_cv.pdf", target="_blank">CV</a> Â· <a href="https://github.com/mpiseno", target="_blank">Github</a>

profile:
  align: right
  image: prof_pic.jpg
  address: >

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---


###### <b>About Me</b>

<p align="justify">
I am a Master's student in Computer Science at Stanford University advised by <a href="https://tml.stanford.edu/people/karen-liu" target="_blank">Karen Liu</a>. I work on robotics and motion prediction in the <a href="https://tml.stanford.edu/" target="_blank">The Movement Lab (TML)</a>. <b>I am currently applying to PhD programs and am interested in joining labs relevant to my research goals discussed below.</b> My research is supported by the <a href="https://en.wikipedia.org/wiki/DoD_NDSEG_Fellowship" target="_blank">National Defense Science and Engineering Graduate (NDSEG)</a> Fellowship.
</p>

<p align="justify">
Prior to joining TML, I was a research assistant in the <a href="https://iliad.stanford.edu/" target="_blank">Intelligent and Interactive Autonomous Systems Group (ILIAD)</a> at Stanford advised by <a href="https://dorsa.fyi/" target="_blank">Dorsa Sadigh</a>, where I worked on language-conditioned imitation learning for robotics. I also spent time at Robotics @ Google (now DeepMind) working on biped locomotion under <a href="https://scholar.google.com/citations?user=neGbgzYAAAAJ&hl=en">Jie Tan</a> and <a href="https://scholar.google.com/citations?user=qOFs67oAAAAJ&hl=en">Atil Iscen</a> in collaboration with the <a href="https://hybrid-robotics.berkeley.edu/people/">Hybrid Robotics Lab</a> at UC Berkeley under <a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath</a>.
</p>

<p align="justify">
<b>Medium-Term Research Goals:</b> I believe humanoids are the best morphology for studying interactive intelligence at scale. I believe this because human motion data is much more scalable than that of other robotic morphologies (e.g. through video-based pose estimation or leveraging gaming inputs for semi-supervision), affording researchers much more data to tackle <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">The Bitter Lesson</a>. Furthermore, motion data combined with other multi-modal data sources grounds semantic knowledge in the physical world, which Large Language Model (LLMs) planners struggle with on their own. Thus, I am interested in combining human motion data with other data sources at scale to create a Large Motion Model (LMM), and studying its emergent effects on general intelligence.
</p>

<!-- <p align="justify">
Outside research, I co-created <a href="https://www.popcornapp.io/" target="_blank">Popcorn</a>, a Q&A platform for classes.
</p> -->

<!-- <p align="justify">
Prior to Stanford, I attended the University of Georgia for one year before transferring to Georgia Tech, where I completed my Bachelors in Computer Science and a minor in Mathematics. I TAed <a href="https://www.cc.gatech.edu/classes/AY2021/cs7643_fall/" target="_blank">CS 7643</a> (Deep Learning) at Georgia Tech for two years.
</p> -->

