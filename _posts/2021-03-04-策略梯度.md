---
layout: post
title: 策略梯度
date: 2021-03-04 11:12:00-0400
description: A mediocre attempt to apply my Chinese knowledge to a technical topic
released: false
---



最近我想学更多的中文技术题目为了扩大我的中文。我也用中文写这个教程因为我想练习中文。我也希望这会是一个好策略梯度解释。


## 策略梯度是什么?

策略梯度是一个最大化期望回报的办法 $$\mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]$$。我们想最大化的目标函数是 $$J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)]$$。我们计算目标函数后，可以用梯度上升方法。

$$\theta_{k+1} = \theta_{k} + \alpha \nabla J(\pi_{\theta})\rvert_{\theta=\theta_{k}}$$

$$\nabla J(\pi_{\theta})$$叫 “策略梯度”。很多算法用策略梯度，比方说 TRPO, PPO, DDPG, 什么的。现在我们会派生策略梯度。

### 派生策略梯度

记起一些东西帮助我们派生：

1. 轨迹的概率 $$\tau$$:
    
    $$
    \begin{align*}
        P(\tau) &= P(s_0, a_1, s_1, ..., s_T, a_T) \\
        &= P(s_0)\prod_{t=0}^{T}P(s_{t+1}|s_t, a_t)\pi_{\theta}(a_t|s_t)
    \end{align*}
    $$

2. 对数导数方法：为一切函数 $$f$$，我们有

    $$\nabla_{\theta}f(x) = f(x)\nabla_{\theta}\log f(x)$$

我们会用这些方法派生策略梯度。

$$
\begin{align*}
    \nabla_{\theta}J(\pi_{\theta}) &= \nabla_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)] \\
    &= \nabla_{\theta} \int_{\tau} P(\tau | \theta) R(\tau) \\
    &= \int_{\tau} \nabla_{\theta} P(\tau | \theta) R(\tau) \\
    &= \int_{\tau} P(\tau | \theta) \nabla_{\theta}\log P(\tau | \theta) R(\tau) \quad \text{对数导数方法} \\
    &= \mathbb{E}_{\tau \sim \pi_{\theta}}[\nabla_{\theta}\log P(\tau | \theta) R(\tau)] \quad \text{期望} \\
    &= \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_{t=0}^{T}\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)R(\tau)]
\end{align*}
$$

最后个部分是因为：

$$
\begin{align*}
    \nabla_{\theta}\log P(\tau | \theta) &= \nabla_{\theta}\log P(s_0) + \sum_{t=0}^{T}\bigg(\nabla_{\theta}\log P(s_{t+1} | s_t, a_t) + \nabla_{\theta}\log \pi_{\theta}(a_t | s_t)\bigg) \\
    &= 0 + \sum_{t=0}^{T}\bigg(0 + \nabla_{\theta}\log \pi_{\theta}(a_t | s_t)\bigg) \\
    &= \sum_{t=0}^{T}\nabla_{\theta}\log \pi_{\theta}(a_t | s_t)
\end{align*}
$$

### 估计策略梯度

为了计算策略梯度，我们得收集一群轨迹 $$\mathcal{D}= \{\tau\}_{i=1, ..., N}$$。那可以算预计的策略梯度。

$$
\begin{equation}
    \hat{g} = \frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}}\sum_{t=0}^{T} \nabla_{\theta}\log \pi_{\theta}(a_t | s_t)R(\tau)
\end{equation}
$$

然后我们在梯度上升方法里用策略梯度估计。

$$\theta_{k+1} = \theta_{k} + \alpha \hat{g}$$


## 基准

一切只依靠状态的函数 $$b(s_t)$$ 有一个特定属性。

$$
\begin{equation}
    \mathbb{E}_{a_t,s_t \sim \pi_{\theta}}[\nabla_{\theta}\log \pi_{\theta}(a_t|s_t)b(s_t)] = 0
\end{equation}
$$

用这种方法的函数叫 “基准函数”。选 $$b(s_t) = V^{\pi_{\theta}}(s_t)$$ 可能会减少策略梯度估计（$$\hat{g}$$）的方差，提高策略梯度的效率。其他选择可以用优势函数：

$$A^{\pi_{\theta}}(s_t, a_t) = Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)$$

然后，完全的目标函数是：

$$
\begin{equation}
    \nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}}\bigg[\sum_{t=0}^{T} \nabla_{\theta}\log \pi_{\theta}(a_t | s_t)b(s_t)\bigg]
\end{equation}
$$

谢谢你们念我的策略梯度解释。我希望我的解释帮助了你们。
